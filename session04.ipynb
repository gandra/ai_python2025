{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e4e71d",
   "metadata": {},
   "source": [
    "# GenAI/RAG in Python 2025\n",
    "\n",
    "## Session 04. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b04159b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5d4d3",
   "metadata": {},
   "source": [
    "### 1. Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9d3f21",
   "metadata": {},
   "source": [
    "Required: `pip install pip install sentence-transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eede4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"_data/italian_recipes_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "083efef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>receipt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BROTH OR SOUP STOCK</td>\n",
       "      <td>(Brodo) To obtain good broth the meat must be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAD SOUP</td>\n",
       "      <td>(Panata) This excellent and nutritious soup is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GNOCCHI</td>\n",
       "      <td>This is an excellent soup, but as it requires ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VEGETABLE SOUP</td>\n",
       "      <td>(Zuppa Sante) Any kind of vegetables may be us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QUEEN'S SOUP</td>\n",
       "      <td>(Zuppa Regina) This is made with the white mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>LEMON ICE</td>\n",
       "      <td>(Gelato di limone) Granulated sugar, 3/4 lb. W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>STRAWBERRY ICE</td>\n",
       "      <td>(Gelato di fragola) Ripe strawberries, 3/4 lb....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>ORANGE ICE</td>\n",
       "      <td>(Gelato di aranci) Four big oranges. One lemon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>PISTACHE ICE CREAM</td>\n",
       "      <td>(Gelato di pistacchi) Milk, one quart. Sugar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>TUTTI FRUTTI</td>\n",
       "      <td>To make this ice a special ice cream mold is n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                            receipt\n",
       "0    BROTH OR SOUP STOCK  (Brodo) To obtain good broth the meat must be ...\n",
       "1             BREAD SOUP  (Panata) This excellent and nutritious soup is...\n",
       "2                GNOCCHI  This is an excellent soup, but as it requires ...\n",
       "3         VEGETABLE SOUP  (Zuppa Sante) Any kind of vegetables may be us...\n",
       "4           QUEEN'S SOUP  (Zuppa Regina) This is made with the white mea...\n",
       "..                   ...                                                ...\n",
       "215            LEMON ICE  (Gelato di limone) Granulated sugar, 3/4 lb. W...\n",
       "216       STRAWBERRY ICE  (Gelato di fragola) Ripe strawberries, 3/4 lb....\n",
       "217           ORANGE ICE  (Gelato di aranci) Four big oranges. One lemon...\n",
       "218   PISTACHE ICE CREAM  (Gelato di pistacchi) Milk, one quart. Sugar, ...\n",
       "219         TUTTI FRUTTI  To make this ice a special ice cream mold is n...\n",
       "\n",
       "[220 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943f811",
   "metadata": {},
   "source": [
    "**Behind the scenes:** The model `all-MiniLM-L6-v2` is a distilled transformer that produces a 384-dimensional embedding for any given sentence or paragraph. Sentence-Transformers handles all the preprocessing (like tokenization) and the heavy lifting of the neural network internally. The resulting embeddings can be used for similarity comparisons, clustering, etc. The library abstracts away the complexity, letting us get embeddings in just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c0cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goransm/Work/___DataKolektiv/_EDU/__EDU2025/_ai/ai_python2025/aipy_venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding for first recipe (truncated to 10 dims): [-0.09900078177452087, -0.003846808336675167, 0.01569218561053276, 0.026051921769976616, -0.07733330875635147, -0.049046289175748825, -0.007966993376612663, -0.003884089644998312, 0.01709478534758091, -0.12944228947162628] ...\n",
      "Embedding length: 384\n"
     ]
    }
   ],
   "source": [
    "# Import the SentenceTransformer class from the sentence_transformers library\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained sentence embedding model (this will download the model if not cached)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Use the model to encode each recipe's text into an embedding vector\n",
    "# We'll create a new column 'embedding' with the resulting list of floats for each recipe.\n",
    "df['embedding'] = df['receipt'].apply(lambda text: model.encode(text).tolist())\n",
    "\n",
    "# Let's print the first recipe's embedding (truncated) and its length to verify\n",
    "print(\"Sample embedding for first recipe (truncated to 10 dims):\", df['embedding'][0][:10], \"...\")\n",
    "print(\"Embedding length:\", len(df['embedding'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37fb73f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>receipt</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BROTH OR SOUP STOCK</td>\n",
       "      <td>(Brodo) To obtain good broth the meat must be ...</td>\n",
       "      <td>[-0.09900078177452087, -0.003846808336675167, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BREAD SOUP</td>\n",
       "      <td>(Panata) This excellent and nutritious soup is...</td>\n",
       "      <td>[-0.05840875208377838, 0.019056806340813637, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GNOCCHI</td>\n",
       "      <td>This is an excellent soup, but as it requires ...</td>\n",
       "      <td>[-0.03781914338469505, -0.01353275217115879, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VEGETABLE SOUP</td>\n",
       "      <td>(Zuppa Sante) Any kind of vegetables may be us...</td>\n",
       "      <td>[-0.09221702814102173, 0.09501173347234726, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>QUEEN'S SOUP</td>\n",
       "      <td>(Zuppa Regina) This is made with the white mea...</td>\n",
       "      <td>[-0.07619944959878922, -0.03227389231324196, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>LEMON ICE</td>\n",
       "      <td>(Gelato di limone) Granulated sugar, 3/4 lb. W...</td>\n",
       "      <td>[-0.05859484523534775, -0.022969068959355354, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>STRAWBERRY ICE</td>\n",
       "      <td>(Gelato di fragola) Ripe strawberries, 3/4 lb....</td>\n",
       "      <td>[-0.016838310286402702, -0.03356937691569328, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>ORANGE ICE</td>\n",
       "      <td>(Gelato di aranci) Four big oranges. One lemon...</td>\n",
       "      <td>[-0.02825223095715046, 0.030553115531802177, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>PISTACHE ICE CREAM</td>\n",
       "      <td>(Gelato di pistacchi) Milk, one quart. Sugar, ...</td>\n",
       "      <td>[-0.015352049842476845, -0.05267338082194328, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>TUTTI FRUTTI</td>\n",
       "      <td>To make this ice a special ice cream mold is n...</td>\n",
       "      <td>[-0.015535758808255196, -0.05635127052664757, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                            receipt  \\\n",
       "0    BROTH OR SOUP STOCK  (Brodo) To obtain good broth the meat must be ...   \n",
       "1             BREAD SOUP  (Panata) This excellent and nutritious soup is...   \n",
       "2                GNOCCHI  This is an excellent soup, but as it requires ...   \n",
       "3         VEGETABLE SOUP  (Zuppa Sante) Any kind of vegetables may be us...   \n",
       "4           QUEEN'S SOUP  (Zuppa Regina) This is made with the white mea...   \n",
       "..                   ...                                                ...   \n",
       "215            LEMON ICE  (Gelato di limone) Granulated sugar, 3/4 lb. W...   \n",
       "216       STRAWBERRY ICE  (Gelato di fragola) Ripe strawberries, 3/4 lb....   \n",
       "217           ORANGE ICE  (Gelato di aranci) Four big oranges. One lemon...   \n",
       "218   PISTACHE ICE CREAM  (Gelato di pistacchi) Milk, one quart. Sugar, ...   \n",
       "219         TUTTI FRUTTI  To make this ice a special ice cream mold is n...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.09900078177452087, -0.003846808336675167, ...  \n",
       "1    [-0.05840875208377838, 0.019056806340813637, 0...  \n",
       "2    [-0.03781914338469505, -0.01353275217115879, -...  \n",
       "3    [-0.09221702814102173, 0.09501173347234726, -0...  \n",
       "4    [-0.07619944959878922, -0.03227389231324196, -...  \n",
       "..                                                 ...  \n",
       "215  [-0.05859484523534775, -0.022969068959355354, ...  \n",
       "216  [-0.016838310286402702, -0.03356937691569328, ...  \n",
       "217  [-0.02825223095715046, 0.030553115531802177, -...  \n",
       "218  [-0.015352049842476845, -0.05267338082194328, ...  \n",
       "219  [-0.015535758808255196, -0.05635127052664757, ...  \n",
       "\n",
       "[220 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5e1260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0afc6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8cd8f",
   "metadata": {},
   "source": [
    "About Sentence Transformers:\n",
    "\n",
    "- [SentenceTransformers Documentation](https://www.sbert.net/)\n",
    "- [List of all 17059 (currently available and growing)](https://huggingface.co/models?library=sentence-transformers) embedding models in Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3913634d",
   "metadata": {},
   "source": [
    "### 2. Method 2: spaCy (Pre-trained GloVe Vectors via spaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53e296",
   "metadata": {},
   "source": [
    "About this method: [spaCy](https://spacy.io/) is a popular open-source NLP library in Python.\n",
    "\n",
    "Among its many features (like part-of-speech tagging, named entity recognition, etc.), spaCy includes pre-trained word vectors for some models. We will use spaCy's English medium model (en_core_web_md), which has 300-dimensional GloVe vectors for words. spaCy can provide a vector for an entire document (in our case, a recipe's text) by averaging the vectors of the words in the text\n",
    "\n",
    "This is a simpler, more lightweight approach than transformers. It may not capture context as well as Sentence-Transformers, but it's fast and easy to use.\n",
    "\n",
    "Required installation: we need to install spacy and download the English model.\n",
    "\n",
    "```\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_md\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d330f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample spaCy embedding for first recipe (10 dims): [-0.6954594254493713, 0.2128593772649765, -0.14669853448867798, -0.03319728374481201, -0.07744687795639038, 0.11704789847135544, -0.0851106271147728, -0.11118240654468536, 0.05072933807969093, 1.7072911262512207] ...\n",
      "Embedding length: 300\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the medium English model in spaCy (this takes a moment to load the model into memory)\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define a function that returns the document vector for a given text\n",
    "def get_doc_embedding(text):\n",
    "    doc = nlp(text)                # Process the text with spaCy (tokenization, etc.)\n",
    "    vector = doc.vector            # The document's vector (average of token vectors for this model)\n",
    "    return vector.tolist()         # Convert the vector (NumPy) to a list of floats\n",
    "\n",
    "# Apply this function to each recipe in the DataFrame to create a new 'embedding' column\n",
    "df['embedding'] = df['receipt'].apply(get_doc_embedding)\n",
    "\n",
    "# Print an example embedding and its length for verification\n",
    "print(\"Sample spaCy embedding for first recipe (10 dims):\", df['embedding'][0][:10], \"...\")\n",
    "print(\"Embedding length:\", len(df['embedding'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b32a8363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e507e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f79ec9",
   "metadata": {},
   "source": [
    "### 3. Gensim Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530ec99",
   "metadata": {},
   "source": [
    "In this approach, we'll create embeddings by training our own `Doc2Vec` model using the [Gensim](https://radimrehurek.com/gensim/) library. Gensim is a toolkit for topic modeling and vector space algorithms (includes Word2Vec, Doc2Vec, LDA, etc.). Doc2Vec (also known as \"Paragraph Vector\") is an algorithm introduced by Le and Mikolov (2014) that learns vector representations for entire documents, beyond just words. It is an extension of `Word2Vec` for larger text segments. The idea is to train a neural network on our corpus such that each document is assigned a vector that helps predict its words After training, documents with similar content should end up with similar vectors in this learned vector space.\n",
    "\n",
    "Unlike the previous two methods, here we will **train a model on our specific dataset** (the Italian recipes). This means the embeddings might capture themes specific to our corpus (e.g., ingredients or cooking terms) but it also means we need to do a bit of setup and the results can vary based on training parameters. We'll keep it simple and use a small vector size for demonstration.\n",
    "\n",
    "Required installation: we need gensim for this method.\n",
    "\n",
    "```\n",
    "pip install gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Prepare training data for Doc2Vec:\n",
    "documents = []\n",
    "for idx, text in enumerate(df['receipt']):\n",
    "    # Split the text into words (simple tokenization on whitespace)\n",
    "    words = text.split()  \n",
    "    # Tag each document with a unique ID (here we use the index)\n",
    "    documents.append(TaggedDocument(words=words, tags=[idx]))\n",
    "\n",
    "# Initialize and train the Doc2Vec model on our documents\n",
    "# We'll use a small vector size (e.g., 50 dimensions) for speed, and train for a few epochs.\n",
    "\n",
    "model = Doc2Vec(vector_size=50, window=5, min_count=2, workers=4, epochs=40)\n",
    "model.build_vocab(documents)          # Build vocabulary from our data\n",
    "model.train(documents, total_examples=len(documents), epochs=model.epochs)\n",
    "\n",
    "# Once trained, the model.dv (document vectors) holds the learned embeddings for each document.\n",
    "# Let's collect the vectors for each recipe in order:\n",
    "doc_vectors = [model.dv[idx] for idx in range(len(documents))]\n",
    "\n",
    "# Convert each vector (NumPy array) to list, then add to DataFrame\n",
    "doc_vectors = [list(vec) for vec in doc_vectors]\n",
    "df['embedding'] = doc_vectors\n",
    "\n",
    "# Print the first recipe's embedding (first 10 dims) and length\n",
    "print(\"Sample Doc2Vec embedding for first recipe (10 dims):\", df['embedding'][0][:10], \"...\")\n",
    "print(\"Embedding length:\", len(df['embedding'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f203fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac398e5",
   "metadata": {},
   "source": [
    "What the code does:\n",
    "\n",
    "- We prepare the data for training. Gensim's `Doc2Vec` requires input as a list of `TaggedDocument` objects, where each `TaggedDocument` is basically a list of words plus a tag (an ID) for the document. We loop through each recipe text, split it into words (note: this is a simple tokenization; for better results one might want to use a smarter tokenizer to handle punctuation, lowercasing, etc., but this simple split suffices for our example). We tag each document with its index idx.\n",
    "\n",
    "- We initialize a `Doc2Vec model`. **Key parameters:**\n",
    "\n",
    "- `vector_size=50`: This sets the embedding dimensionality to 50. (You can increase this for more complex patterns, but 50 is okay for demonstration.)\n",
    "- `window=5`: The context window size (how many words before/after to consider in the training context for predicting words).\n",
    "`min_count=2`: Ignore words that appear less than 2 times (this helps ignore very rare words).\n",
    "`workers=4`: Number of parallel threads to use (adjust based on your CPU cores).\n",
    "`epochs=40`: How many iterations (epochs) to train for. More epochs = more training but also more time.\n",
    "\n",
    "- We call `build_vocab(documents)` to prepare the vocabulary.\n",
    "\n",
    "- Then we train the model on our documents with `model.train(...)`. This may take some seconds depending on corpus size and parameters. (Our dataset is not very large – 220 recipes – so this should be quite fast even on a CPU.)\n",
    "\n",
    "- After training, `model.dv` contains the learned document vectors. We extract each vector by index and store them in a list `doc_vectors`.\n",
    "\n",
    "- We convert each vector to a regular list of floats (since Gensim gives us a numpy array for each vector).\n",
    "\n",
    "- We assign this list of lists to `df['embedding']`. Now each recipe in the DataFrame has a 50-dimensional embedding learned specifically from our dataset.\n",
    "\n",
    "- We print the first embedding (truncated) and its length to verify it's 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['embedding'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipy_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
