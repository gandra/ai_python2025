{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e4e71d",
   "metadata": {},
   "source": [
    "# GenAI/RAG in Python 2025\n",
    "\n",
    "## Session 02. Expanding the Basic RAG Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04159b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756641c",
   "metadata": {},
   "source": [
    "## 1. Let's grab some text... Italian cuisine, for example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc71709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "file_path = \"_data/italian_recipes_clean.csv\"\n",
    "\n",
    "# Load the CSV into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display some basic information\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbbd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"receipt\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e432c",
   "metadata": {},
   "source": [
    "### We would like to build a system that...\n",
    "\n",
    "(1) Takes user input in the form of a question (e.g. \"I'd like to cook something with carrots\"), (2) performs a similarity search across the recipes in the `df` DataFrame, (3) obtains the most similar five recipes, lists them, and (4) combines them with a prompt sent to ChatGPT to shape the final response that is shared with the user.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d704f",
   "metadata": {},
   "source": [
    "## 2. Vector embeddings for similarity search\n",
    "\n",
    "All recipes must be embedded in order to be prepared for similarity search.\n",
    "\n",
    "We will use OpenAI's embedding models this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key (ensure OPENAI_API_KEY is set in your environment)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Instantiate the OpenAI client with your API key  \n",
    "client = OpenAI(api_key=api_key)                    \n",
    "\n",
    "# Select the embedding model to use (as per OpenAI docs)  \n",
    "model_name = \"text-embedding-3-small\"      \n",
    "\n",
    "# Prepare a list to collect embedding vectors  \n",
    "embeddings = []                            \n",
    "\n",
    "# Iterate over each row in your DataFrame `df`  \n",
    "for idx, row in df.iterrows():\n",
    "    # grab the receipt text for this row              \n",
    "    text = row[\"receipt\"]  \n",
    "    # If it's not a valid string, skip embedding  \n",
    "    if not isinstance(text, str) or text.strip() == \"\":  \n",
    "        embeddings.append(None)             \n",
    "        continue                            \n",
    "\n",
    "    # Call the embeddings endpoint on the client  \n",
    "    resp = client.embeddings.create(        \n",
    "        model=model_name,                   \n",
    "        input=[text]                        \n",
    "    )                                     \n",
    "\n",
    "    # Extract the embedding vector from the response object  \n",
    "    emb = resp.data[0].embedding            \n",
    "\n",
    "    # Append that embedding vector to our list  \n",
    "    embeddings.append(emb)                  \n",
    "\n",
    "# After the loop, assign embeddings list to a new DataFrame column  \n",
    "df[\"embedding\"] = embeddings               \n",
    "\n",
    "# Show first few rows to verify  \n",
    "df.head()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083efef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ee7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e125a69",
   "metadata": {},
   "source": [
    "## 3. Now we need a user input..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f86021",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = \"\"\"\n",
    "Hi! I‚Äôd like to cook a good Italian dish for lunch! I have potatoes, carrots, \n",
    "rosemary, and pork. Can you recommend a recipe and help me a bit with \n",
    "preparation tips?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945843b",
   "metadata": {},
   "source": [
    "... and of course we need an embedding of `user_text` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2fc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.embeddings.create(        \n",
    "        model=model_name,                   \n",
    "        input=[user_text]                        \n",
    "    )\n",
    "user_query = resp.data[0].embedding\n",
    "\n",
    "print(type(user_query))\n",
    "print(len(user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eccab36",
   "metadata": {},
   "source": [
    "## 4a. Find the most suitable examples that match the user input: Cosine Distance (Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f673000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy has a function to compute cosine distance: cosine()\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Compute similarity scores: similarity = 1 ‚àí cosine_distance\n",
    "scores = []\n",
    "for emb in df[\"embedding\"]:\n",
    "    if emb is None:\n",
    "        scores.append(-1.0)\n",
    "    else:\n",
    "        # np.array is a vector data type that scipy wants to see\n",
    "        # in place of a list \n",
    "        scores.append(1.0 - cosine(np.array(emb), np.array(user_query)))\n",
    "\n",
    "# Get top 5 indices\n",
    "top5 = np.argsort(scores)[-5:]\n",
    "# N.B. np.argsort(scores) ‚Äî returns an array of indices that would \n",
    "# sort scores in ascending order. \n",
    "# [-5:] ‚Äî takes the last 5 indices from that sorted‚Äêindices array. \n",
    "# Since the full array is in ascending order, its last 5 indices correspond to \n",
    "# the 5 highest scores.\n",
    "\n",
    "# Build a single output string with titles and recipes\n",
    "output_lines = []\n",
    "for i in top5:\n",
    "    title = df.iloc[i][\"title\"]\n",
    "    recipe = df.iloc[i][\"receipt\"]\n",
    "    output_lines.append(f\"{title}:\\n{recipe}\")\n",
    "prompt_recipes = \"\\n\\n\".join(output_lines)\n",
    "\n",
    "print(prompt_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0bd07b",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "$$\n",
    "\\cos\\theta = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\|\\;\\|\\mathbf{b}\\|}\n",
    "= \\frac{\\sum_{i=1}^n a_i\\,b_i}{\\sqrt{\\sum_{i=1}^n a_i^2}\\;\\sqrt{\\sum_{i=1}^n b_i^2}}\n",
    "$$\n",
    "\n",
    "A common definition of **cosine similarity** is:\n",
    "\n",
    "$$\n",
    "d_{\\text{cos}}(\\mathbf{a},\\mathbf{b}) = 1 - \\cos\\theta\n",
    "$$\n",
    "\n",
    "- In text / embedding applications, higher cosine similarity (or lower cosine distance) means vectors are more semantically aligned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c8d45",
   "metadata": {},
   "source": [
    "## 4b. More distance/similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d411a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9538c",
   "metadata": {},
   "source": [
    "### *Cosine* \n",
    "\n",
    "$$\n",
    "\\text{sim}_{\\cos}(\\mathbf{a}, \\mathbf{b}) =\n",
    "\\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69bb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0.1, 0.3, 0.5, 0.0])\n",
    "b = np.array([0.2, 0.1, 0.4, 0.3])\n",
    "\n",
    "cos_sim = cosine(a, b)\n",
    "\n",
    "print(\"1. Cosine similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5220c79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### *Understanding Cosine Similarity*\n",
    "\n",
    "Cosine similarity is one of the most popular ways to measure how **similar two documents (or vectors)** are ‚Äî especially in **text retrieval** or **semantic search**.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imagine every document as a **point** (or arrow) in a multi-dimensional space.  If two documents use **similar words in similar proportions**, their arrows point in roughly the same direction ‚Äî even if one is longer (has more words). Cosine similarity measures **how close their directions are**, not their lengths.\n",
    "\n",
    "---\n",
    "\n",
    "### The Formula\n",
    "\n",
    "For two vectors **a** and **b**:\n",
    "\n",
    "$$\n",
    "\\\n",
    "\\text{similarity}_{\\cos}(\\mathbf{a}, \\mathbf{b}) =\n",
    "\\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\|}\n",
    "\\\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\,\\mathbf{a}\\cdot\\mathbf{b}\\,$ is the **dot product** ‚Äî how much the two vectors align.\n",
    "- $\\,\\lVert \\mathbf{a}\\rVert$ and $\\,\\lVert \\mathbf{b}\\rVert$ are their **magnitudes (lengths)**.\n",
    "- The result is a number between **‚àí1 and 1**, but in most IR applications (non-negative vectors) it ranges from **0 to 1**:\n",
    "  - **1 ‚Üí** same direction (identical content)\n",
    "  - **0 ‚Üí** completely different\n",
    "  - **values in between ‚Üí** partial similarity\n",
    "\n",
    "In `scipy.spatial.distance`, the function `distance.cosine(a, b)` returns the **cosine *distance*** ‚Äî not the similarity.\n",
    "\n",
    "$$\n",
    "d_{\\cos}(\\mathbf{a}, \\mathbf{b}) = 1 - \n",
    "\\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\|}\n",
    "$$\n",
    "\n",
    "- `distance.cosine(a, b)` ‚Üí **distance** (0 means identical, 1 means orthogonal)  \n",
    "- `1 - distance.cosine(a, b)` ‚Üí **similarity** (1 means identical, 0 means orthogonal)\n",
    "\n",
    "In short:\n",
    "> **Cosine distance** measures *how far apart* two vectors are.  \n",
    "> **Cosine similarity** measures *how aligned* they are.\n",
    "\n",
    "**Range:**\n",
    "- For **non-negative vectors** the cosine similarity ‚àà [0, 1], so the **distance ‚àà [0, 1]**.  \n",
    "- If vectors can have **negative components**, similarity ‚àà [‚àí1, 1], so **distance ‚àà [0, 2]**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce1260",
   "metadata": {},
   "source": [
    "### *Euclidean Distance*\n",
    "\n",
    "$$\n",
    "d_{\\text{Euc}}(\\mathbf{a}, \\mathbf{b}) =\n",
    "\\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "euc_dist = distance.euclidean(a, b)\n",
    "print(\"2. Euclidean distance:\", euc_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5849245",
   "metadata": {},
   "source": [
    "### *Understanding Euclidean Distance (L2 norm)*\n",
    "\n",
    "Euclidean distance is the most familiar way to measure how **far apart** two things are ‚Äî  \n",
    "it‚Äôs the same idea as measuring the straight-line distance between two points in space.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imagine two documents, or two data points, as **positions in space**.  \n",
    "Each feature (like a word weight or an embedding dimension) is one coordinate axis.  \n",
    "The Euclidean distance tells us how long the straight line is between these two points.\n",
    "\n",
    "If two points are close together, their values are similar.  \n",
    "If they‚Äôre far apart, the difference between them is large.\n",
    "\n",
    "---\n",
    "\n",
    "### The Formula\n",
    "\n",
    "For two vectors **a** and **b** with $n$ components:\n",
    "\n",
    "$$\n",
    "d_{\\text{Euc}}(\\mathbf{a}, \\mathbf{b}) =\n",
    "\\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Breaking it Down\n",
    "\n",
    "- $\\,a_i - b_i\\,$ measures how much the two vectors differ on each coordinate.  \n",
    "- Squaring each difference, $(a_i - b_i)^2$, keeps all values positive.  \n",
    "- Summing them up adds all the little differences together.  \n",
    "- Taking the square root gives the actual straight-line distance.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "- **Small distance ‚Üí** the two items are very similar.  \n",
    "- **Large distance ‚Üí** they are quite different.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f74a7e",
   "metadata": {},
   "source": [
    "### *Manhattan Distance*\n",
    "\n",
    "$$\n",
    "d_{\\text{Man}}(\\mathbf{a}, \\mathbf{b}) =\n",
    "\\sum_{i=1}^{n} |a_i - b_i|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4f2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "man_dist = distance.cityblock(a, b)\n",
    "print(\"Manhattan distance:\", man_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c5418",
   "metadata": {},
   "source": [
    "### Manhattan Distance (L1 Norm)\n",
    "\n",
    "Manhattan distance measures how far apart two vectors are by **summing the absolute differences** of their coordinates ‚Äî  like moving through a city grid where you can only go along streets, not diagonally.\n",
    "\n",
    "$$\n",
    "d_{\\text{Man}}(\\mathbf{a}, \\mathbf{b}) =\n",
    "\\sum_{i=1}^{n} |a_i - b_i|\n",
    "$$\n",
    "\n",
    "**Range:**\n",
    "- Minimum: **0** (the vectors are identical).  \n",
    "- No fixed maximum ‚Äî it grows with the number of dimensions and the scale of the data.  \n",
    "- If features are normalized to [0, 1], the maximum distance is the **number of dimensions** *n*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49901478",
   "metadata": {},
   "source": [
    "#### 4. Jaccard Similarity (for discrete features)\n",
    "$$\n",
    "\\text{sim}_{\\text{Jaccard}}(A, B) =\n",
    "\\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_bin = np.array([1, 0, 1, 0])\n",
    "b_bin = np.array([1, 1, 0, 0])\n",
    "jac_sim = 1 - distance.jaccard(a_bin, b_bin)\n",
    "print(\"Jaccard similarity:\", jac_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea386c",
   "metadata": {},
   "source": [
    "### Jaccard Similarity (for Discrete Features)\n",
    "\n",
    "The **Jaccard similarity** measures how much two sets (or binary feature vectors) **overlap**.\n",
    "\n",
    "It‚Äôs ideal for comparing **discrete data**, such as:\n",
    "\n",
    "- tags assigned to items,\n",
    "- binary attributes (e.g., feature present / not present).\n",
    "\n",
    "---\n",
    "\n",
    "### The Formula\n",
    "\n",
    "For two sets $A$ and $B$:\n",
    "\n",
    "$$\n",
    "\\text{similarity}_{\\text{Jaccard}}(A, B) =\n",
    "\\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "- $|A \\cap B|$ ‚Äî number of shared elements (the overlap)  \n",
    "- $|A \\cup B|$ ‚Äî number of unique elements across both sets  \n",
    "\n",
    "The result is a value between **0 and 1**:\n",
    "- **1 ‚Üí** sets are identical  \n",
    "- **0 ‚Üí** sets share nothing in common  \n",
    "\n",
    "---\n",
    "\n",
    "### Distance vs. Similarity\n",
    "\n",
    "The **Jaccard distance** is simply the inverse measure:\n",
    "\n",
    "$$\n",
    "d_{\\text{Jaccard}}(A, B) = 1 - \\text{similarity}_{\\text{Jaccard}}(A, B)\n",
    "$$\n",
    "\n",
    "So:\n",
    "- **Similarity** ‚Üí how much two sets *share*  \n",
    "- **Distance** ‚Üí how much they *differ*  \n",
    "\n",
    "In short:\n",
    "> **Jaccard similarity** counts the overlap.  \n",
    "> **Jaccard distance** counts the non-overlap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59183b",
   "metadata": {},
   "source": [
    "## 5. Handling Discrete Features in Information (Document) Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b881d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "file_path = \"_data/italian_recipes_features.csv\"\n",
    "\n",
    "# Load the CSV into a Pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display some basic information\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d695b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aeca7b",
   "metadata": {},
   "source": [
    "### User Input\n",
    "\n",
    "> ‚ÄúHey, I‚Äôm in the mood for something hearty but not too complicated. I‚Äôd love to cook a traditional Italian pasta dish, maybe with a rich tomato sauce, some garlic and olive oil, and a bit of Parmesan on top. I prefer something savory, not sweet ‚Äî and ideally something that‚Äôs cooked on the stove, not baked. Any classic recipes you can recommend?‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_request = \"\"\"\n",
    "Hey, I‚Äôm in the mood for something hearty but not too complicated. \n",
    "I‚Äôd love to cook a traditional Italian pasta dish, maybe with a rich tomato sauce, \n",
    "some garlic and olive oil, and a bit of Parmesan on top. \n",
    "I prefer something savory, not sweet ‚Äî and ideally something that‚Äôs cooked on the stove, not baked. \n",
    "Any classic recipes you can recommend?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba55c161",
   "metadata": {},
   "source": [
    "### Tag User Input (via LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb17162",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are given a user's request.  \n",
    "Based on the request, output ONLY a valid JSON object with the following binary features, \n",
    "where each value must be either 0 or 1:\n",
    "\n",
    "[\n",
    "  \"is_soup_broth\", \"is_pasta\", \"is_rice\", \"is_meat_dish\", \"is_fish_dish\", \"is_egg_dish\", \n",
    "  \"is_vegetable_dish\", \"is_dessert\", \"contains_pasta\", \"contains_rice\", \"contains_meat\", \n",
    "  \"contains_fish_seafood\", \"contains_egg\", \"contains_cheese\", \"contains_tomato\", \n",
    "  \"contains_olive_oil\", \"contains_garlic\", \"contains_wine\", \"contains_herbs\",\n",
    "  \"is_boiled\", \"is_baked\", \"is_fried\", \"is_grilled\", \"is_raw_preparation\", \n",
    "  \"is_sauce_based\", \"is_slow_cooked\", \"has_stuffing\", \"served_with_sauce\", \"is_soup_like\", \n",
    "  \"is_bread_based\", \"is_spicy\", \"is_savory\", \"is_sweet\", \"contains_citrus\",\n",
    "  \"mentions_region\", \"mentions_dialect_term\", \"is_classic_named_dish\"\n",
    "]\n",
    "\n",
    "Do not include explanations, extra text, or any other formatting‚Äîonly \n",
    "the JSON object with keys and binary (0/1) values for each of the above features.\n",
    "\n",
    "User request: {user_request}\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Italian cooking assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    # Force JSON response\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "tags = response.choices[0].message.content\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cb6cc",
   "metadata": {},
   "source": [
    "#### Parse JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# --- Step 1: Parse JSON ---\n",
    "feature_dict = json.loads(tags)\n",
    "print(feature_dict)\n",
    "# Extract feature columns (order matters)\n",
    "feature_cols = list(feature_dict.keys())\n",
    "# --- Step 2: Convert LLM output to vector ---\n",
    "user_vector = pd.Series(feature_dict, index=feature_cols).astype(int)\n",
    "print(user_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570f5a4",
   "metadata": {},
   "source": [
    "#### Compare to `df` to find five similar examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571757e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Extract binary feature matrix from df ---\n",
    "feature_matrix = df[feature_cols].astype(int).values\n",
    "\n",
    "# --- Step 4: Compute Jaccard similarity using scipy ---\n",
    "# distance.jaccard returns distance = 1 - similarity\n",
    "jaccard_similarities = [1 - distance.jaccard(user_vector, row) for row in feature_matrix]\n",
    "\n",
    "# --- Step 5: Find Top 5 most similar recipes ---\n",
    "df['jaccard_similarity'] = jaccard_similarities\n",
    "top5 = df.nlargest(5, 'jaccard_similarity')\n",
    "\n",
    "print(top5[['title', 'receipt', 'jaccard_similarity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46eaf9",
   "metadata": {},
   "source": [
    "## 6. (In)determinism in LLMs: `temperature` and `top_p` parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_recipes = \"\"\n",
    "\n",
    "for _, row in top5.iterrows():\n",
    "    prompt_recipes += f\"{row['title'].strip()}\\n\"\n",
    "    prompt_recipes += f\"{row['receipt'].strip()}\\n\\n\"\n",
    "\n",
    "prompt_recipes = prompt_recipes.strip()\n",
    "\n",
    "print(prompt_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f59cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a helpful Italian cooking assistant.  \n",
    "Here are some recipe examples I found that may or may not be relevant to the user's request:\n",
    "\n",
    "{prompt_recipes}\n",
    "\n",
    "User‚Äôs question: \"{user_request}\"\n",
    "\n",
    "From the examples above:\n",
    "1. Determine which recipes are *relevant* to what the user asked and which are not.\n",
    "2. Discard or ignore irrelevant ones, and focus on relevant ones.\n",
    "3. For each relevant example, rephrase the recipe in a more narrative, \n",
    "conversational style, adding cooking tips, alternative ingredients, variations, \n",
    "or suggestions.\n",
    "4. Then produce a final response to the user: a narrative that weaves \n",
    "together those enhanced recipes (titles + steps + tips) in an engaging way.\n",
    "5. Don't forget to use the original titles of the recipes.\n",
    "6. Advise on more than one recipe - if there are more than one relevant!\n",
    "\n",
    "Do not just list recipes ‚Äî tell a story, connect to the user's question, \n",
    "and use the examples as inspirations, but enhance them.  \n",
    "Make sure your response is clear, helpful, and focused on what the user wants.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25dc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",    # or whichever model you prefer\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Italian cooking assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature = 0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "reply_text = response.choices[0].message.content\n",
    "\n",
    "print(user_request)\n",
    "\n",
    "print(reply_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c751eb",
   "metadata": {},
   "source": [
    "### Now... increase the heat (`temperature`)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf898043",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",    # or whichever model you prefer\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Italian cooking assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature = 1.5,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "reply_text = response.choices[0].message.content\n",
    "\n",
    "print(user_request)\n",
    "\n",
    "print(reply_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0bf12c",
   "metadata": {},
   "source": [
    "### Ooops..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",    # or whichever model you prefer\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Italian cooking assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature = .75,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "reply_text = response.choices[0].message.content\n",
    "\n",
    "print(user_request)\n",
    "\n",
    "print(reply_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df4fbd",
   "metadata": {},
   "source": [
    "### One more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",    # or whichever model you prefer\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Italian cooking assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature = 1.25,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "reply_text = response.choices[0].message.content\n",
    "\n",
    "print(user_request)\n",
    "\n",
    "print(reply_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ffffa1",
   "metadata": {},
   "source": [
    "### üîç Definitions\n",
    "\n",
    "* **`temperature`**\n",
    "  This parameter controls how *random or deterministic* the model‚Äôs token-sampling is. At a low temperature (closer to 0), the model becomes very deterministic, tending to pick the highest-probability next tokens consistently. At a higher temperature (closer to 1 or above, depending on model) the probability distribution of possible next tokens is flattened, meaning tokens with lower original probabilities become more likely to be sampled. \n",
    "\n",
    "  From the OpenAI docs:\n",
    "\n",
    "  > ‚ÄúWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.‚Äù\n",
    "\n",
    "* **`top_p`** (also known as *nucleus sampling*)\n",
    "  This parameter sets a threshold on *cumulative probability mass* of tokens to consider. Rather than scaling all probabilities (as temperature does), you sort possible next tokens by their probability, then include the smallest set whose combined probability is ‚â• `top_p`. The next token is sampled from that subset. So if `top_p = 0.9`, you consider the tokens that together account for 90 % of the probability mass and ignore the rest (lowest-probability tokens).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences and Practical Implications\n",
    "\n",
    "* **Mechanism**:\n",
    "\n",
    "  * `temperature` adjusts *how flat or peaked* the probability distribution of next tokens is.\n",
    "  * `top_p` restricts the *space of possible next tokens* to the most probable subset (cumulative mass) and discards the rest.\n",
    "\n",
    "* **Effect on output**:\n",
    "\n",
    "  * A *low* `temperature` tends to yield more predictable, consistent, and ‚Äúsafe‚Äù output.\n",
    "  * A *high* `temperature` yields more varied, creative, and risk-taking output.\n",
    "  * A *low* `top_p` (e.g., 0.1‚Äì0.3) restricts the token selection drastically ‚Üí very focused/deterministic output.\n",
    "  * A *high* `top_p` (e.g., 0.8‚Äì1.0) allows a broader set of token choices ‚Üí more diversity.\n",
    "\n",
    "* **When to use which**:\n",
    "\n",
    "  * If you want **maximum control** and deterministic output (e.g., factual answers, code generation, tables) ‚Üí use low `temperature` (maybe ~0‚Äì0.3) and/or low `top_p`.\n",
    "  * If you want **creativity, variety, open-ended responses** (e.g., story generation, brainstorming) ‚Üí use higher `temperature` (e.g., 0.7‚Äì1.0) and/or higher `top_p`.\n",
    "  * Many developers follow the guideline: **use one of them**, not both aggressively. As the docs note:\n",
    "\n",
    "    > ‚ÄúWe generally recommend altering this or `top_p` but not both.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "* Use **`temperature`** to **scale the randomness** of token selection (how adventurous the model is).\n",
    "* Use **`top_p`** to **limit the pool** of token candidates (how many possible next tokens the model may consider).\n",
    "* Both influence **diversity vs. determinism** of output, but they do so via different mechanisms.\n",
    "* For many use cases, adjusting **one** of them is sufficient (and simpler) rather than tweaking both simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipy_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
