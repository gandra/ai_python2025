{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86e4e71d",
   "metadata": {},
   "source": [
    "# GenAI/RAG in Python 2025\n",
    "\n",
    "## Session 03. \n",
    "\n",
    "## (1) Postgres+pgvector Vector Storage\n",
    "## (2) Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04159b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee61bbd",
   "metadata": {},
   "source": [
    "## 1. Postgres + pgvector as our vector storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836fa16",
   "metadata": {},
   "source": [
    "Install Docker image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b18d3",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "```\n",
    "docker pull pgvector/pgvector:pg16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875aaa0",
   "metadata": {},
   "source": [
    "Run the container:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f1fbbd",
   "metadata": {},
   "source": [
    "```\n",
    "docker run --name ragdb \\\n",
    "  -e POSTGRES_USER=raguser \\\n",
    "  -e POSTGRES_PASSWORD=ragpass \\\n",
    "  -e POSTGRES_DB=ragdb \\\n",
    "  -p 5432:5432 \\\n",
    "  -d pgvector/pgvector:pg16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192d246",
   "metadata": {},
   "source": [
    "| Option                 | Meaning                                                 |\n",
    "| ---------------------- | ------------------------------------------------------- |\n",
    "| `--name ragdb`         | names the container (handy for stopping/starting later) |\n",
    "| `-e POSTGRES_USER`     | creates a default Postgres user                         |\n",
    "| `-e POSTGRES_PASSWORD` | sets that user’s password                               |\n",
    "| `-e POSTGRES_DB`       | creates a database on startup                           |\n",
    "| `-p 5432:5432`         | exposes port 5432 on localhost                          |\n",
    "| `-d`                   | runs in detached (background) mode                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fce370",
   "metadata": {},
   "source": [
    "Postgres + pgvector is now running locally on port `5432`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf00c92",
   "metadata": {},
   "source": [
    "Verify:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14566c11",
   "metadata": {},
   "source": [
    "```\n",
    "docker exec -it ragdb psql -U raguser -d ragdb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc7f84",
   "metadata": {},
   "source": [
    "```\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "\\dx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef15213",
   "metadata": {},
   "source": [
    "```\n",
    "SELECT 'pgvector ready' AS status;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd3744",
   "metadata": {},
   "source": [
    "Stopping / cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda65fef",
   "metadata": {},
   "source": [
    "```\n",
    "docker stop ragdb         # stop the container\n",
    "docker start ragdb        # restart it\n",
    "docker rm -f ragdb        # remove it completely\n",
    "docker volume rm pgdata   # remove stored data (optional)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"_data/italian_recipes_embedded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083efef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f9e88",
   "metadata": {},
   "source": [
    "We want to migrate `df` to Postgres now... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192ccbb",
   "metadata": {},
   "source": [
    "Note: `embedding` is an `object` (a string indeed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8702458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Parse string embeddings into numpy arrays ---\n",
    "df['embedding_vector'] = df['embedding'].apply(\n",
    "    lambda x: np.array(ast.literal_eval(x), dtype=np.float32)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80d894d",
   "metadata": {},
   "source": [
    "Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b14c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fdfca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba95cd",
   "metadata": {},
   "source": [
    "Nope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ast.literal_eval(df['embedding'][0]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30d221",
   "metadata": {},
   "source": [
    "Because:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.literal_eval(df['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cee403",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ast.literal_eval(df['embedding'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d7e8a",
   "metadata": {},
   "source": [
    "Create Postgres table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58493193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Connect to Postgres ---\n",
    "from pgvector.psycopg import register_vector\n",
    "conn = psycopg.connect(\n",
    "    host=\"localhost\",\n",
    "    dbname=\"ragdb\",\n",
    "    user=\"raguser\",\n",
    "    password=\"ragpass\",\n",
    "    port=5432\n",
    ")\n",
    "\n",
    "register_vector(conn)\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ebad2",
   "metadata": {},
   "source": [
    "Drop the `receipts` table if the code was previously executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"DROP TABLE IF EXISTS receipts;\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Create table if not exists ---\n",
    "\n",
    "# - dimensionality\n",
    "dim = len(df['embedding_vector'][0])\n",
    "\n",
    "# - create:\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS receipts (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    title TEXT,\n",
    "    receipt TEXT,\n",
    "    embedding VECTOR({dim})\n",
    ");\n",
    "\"\"\"\n",
    "cur.execute(create_table_sql)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c6fe6",
   "metadata": {},
   "source": [
    "Transfer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6cc621",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    cur.execute(\n",
    "        \"INSERT INTO receipts (title, receipt, embedding) VALUES (%s, %s, %s)\",\n",
    "        (row['title'], row['receipt'], row['embedding_vector'].tolist())\n",
    "    )\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ffe17",
   "metadata": {},
   "source": [
    "Check table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - a simple query to test\n",
    "cur.execute(\"\"\"\n",
    "    SELECT id, title, receipt, embedding\n",
    "    FROM receipts\n",
    "    LIMIT 5;\n",
    "\"\"\")\n",
    "\n",
    "# - fetch results\n",
    "rows = cur.fetchall()\n",
    "colnames = [desc[0] for desc in cur.description]\n",
    "\n",
    "# DataFrame\n",
    "df_test = pd.DataFrame(rows, columns=colnames)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ccd1e0",
   "metadata": {},
   "source": [
    "### Client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key (ensure OPENAI_API_KEY is set in your environment)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Instantiate the OpenAI client with your API key  \n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f86021",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = \"\"\"\n",
    "Hi! I’d like to cook a good Italian dish for lunch! I have potatoes, carrots, \n",
    "rosemary, and pork. Can you recommend a recipe and help me a bit with \n",
    "preparation tips?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945843b",
   "metadata": {},
   "source": [
    "... and of course we need an embedding of `user_text` as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2fc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the embedding model to use (as per OpenAI docs)  \n",
    "model_name = \"text-embedding-3-small\"   \n",
    "\n",
    "resp = client.embeddings.create(        \n",
    "        model=model_name,                   \n",
    "        input=[user_text]                        \n",
    "    )\n",
    "user_query = resp.data[0].embedding\n",
    "\n",
    "print(type(user_query))\n",
    "print(len(user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e7589",
   "metadata": {},
   "source": [
    "Find the most suitable examples that match the user input: Cosine Distance (Similarity) in Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f673000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the retrieval query using pgvector operator\n",
    "# NOTE: ::vector - the \"vector adapter\"\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT id, title, receipt, 1 - (embedding <=> %s::vector) AS similarity\n",
    "FROM receipts\n",
    "ORDER BY embedding <=> %s::vector\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "cur.execute(sql, (user_query, user_query))\n",
    "\n",
    "rows = cur.fetchall()\n",
    "colnames = [desc[0] for desc in cur.description]\n",
    "\n",
    "# - Load into pandas DataFrame\n",
    "prompt_recipes = pd.DataFrame(rows, columns=colnames)\n",
    "\n",
    "# Show results\n",
    "print(prompt_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f8d3b",
   "metadata": {},
   "source": [
    "Nice; clean-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8bd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde09146",
   "metadata": {},
   "source": [
    "Integrate results into the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f24a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a single output string with titles and recipes\n",
    "output_lines = []\n",
    "for _, row in prompt_recipes.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    recipe = row[\"receipt\"]\n",
    "    output_lines.append(f\"{title}:\\n{recipe}\")\n",
    "prompt_recipes = \"\\n\\n\".join(output_lines)\n",
    "print(prompt_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2eedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a helpful Italian cooking assistant.  \n",
    "Here are some recipe examples I found that may or may not be relevant to the user's request:\n",
    "\n",
    "{prompt_recipes}\n",
    "\n",
    "User’s question: \"{user_text}\"\n",
    "\n",
    "From the examples above:\n",
    "1. Determine which recipes are *relevant* to what the user asked and which are not.\n",
    "2. Discard or ignore irrelevant ones, and focus on relevant ones.\n",
    "3. For each relevant example, rephrase the recipe in a more narrative, \n",
    "conversational style, adding cooking tips, alternative ingredients, variations, \n",
    "or suggestions.\n",
    "4. Then produce a final response to the user: a narrative that weaves \n",
    "together those enhanced recipes (titles + steps + tips) in an engaging way.\n",
    "5. Don't forget to use the original titles of the recipes.\n",
    "6. Advise on more than one recipe - if there are more than one relevant!\n",
    "\n",
    "Do not just list recipes — tell a story, connect to the user's question, \n",
    "and use the examples as inspirations, but enhance them.  \n",
    "Make sure your response is clear, helpful, and focused on what the user wants.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497843c",
   "metadata": {},
   "source": [
    "Run prompt, sit and enjoy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd091e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",    # or whichever model you prefer\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Italian cooking assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=5000\n",
    ")\n",
    "\n",
    "reply_text = response.choices[0].message.content\n",
    "print(reply_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccebd05",
   "metadata": {},
   "source": [
    "## 2. Function Calling with OpenAI ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55193684",
   "metadata": {},
   "source": [
    "OpenAI's function calling feature allows ChatGPT to output structured data (like JSON) to call external functions or APIs. This means the model can decide when to use a function you provide, and it will return the function name and arguments it wants to call. The developer's code can then execute that function (e.g., call an external API) and pass the result back to the model. This is useful for retrieving real-time information (like weather or stock prices) or performing computations that the model can't do by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312b80b",
   "metadata": {},
   "source": [
    "### 2.1 The Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b191c1",
   "metadata": {},
   "source": [
    "First, we need an external API that our tool (previously known as: `function` in the OpenAI API) will call. For this educational example, we'll use the `wttr.in` weather API. It's a free API (no API key needed) that returns current weather information for a given location in JSON format.\n",
    "\n",
    "We'll write a Python function `get_current_weather(location)` that calls this API. This function will serve as the \"tool\" that ChatGPT can use via function calling. In a real application, this function might call any external service or perform some calculation. Here, it will fetch weather data and return a simple text summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3056d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the current weather in Celsius for a given location using wttr.in.\n",
    "    Returns a simple summary string.\n",
    "    \"\"\"\n",
    "    url = f\"http://wttr.in/{location}?format=j1\"\n",
    "    data = requests.get(url).json()\n",
    "    current = data[\"current_condition\"][0]\n",
    "    temp_c = current[\"temp_C\"]\n",
    "    desc = current[\"weatherDesc\"][0][\"value\"]\n",
    "    return f\"Temperature: {temp_c}°C, Condition: {desc}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646ec67",
   "metadata": {},
   "source": [
    "### 2.2. Defining the Function Schema for the OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597999a",
   "metadata": {},
   "source": [
    "Now that we have a Python function, we need to tell the OpenAI API about it. We do this by providing a tool schema in our API call. The schema includes the tool name, description, and parameters (with types) that the tool accepts. This information helps the model decide when and how to call the function.\n",
    "\n",
    "We'll prepare a dictionary that follows the OpenAI function calling specification. This will later be passed to the functions parameter of the chat completion API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40047db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function schema for the OpenAI API\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather for a city (Celsius only)\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"City name, e.g. Paris or London\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea711347",
   "metadata": {},
   "source": [
    "This schema is how we register the function with the model. With `functions=[...]` in the API call, GPT-3.5/4 is aware that it has an available tool named \"get_current_weather\" and how to call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d546e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"What's the weather in Paris right now?\"}\n",
    "]\n",
    "\n",
    "# call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# result\n",
    "response_message = response.choices[0].message\n",
    "print(response_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59183576",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = response_message.tool_calls[0].function.arguments\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c46b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(resp))\n",
    "import json\n",
    "args = json.loads(resp)\n",
    "print(type(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fdebd",
   "metadata": {},
   "source": [
    "Find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "weather_conditions = get_current_weather(args['location'])\n",
    "print(weather_conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ebcc90",
   "metadata": {},
   "source": [
    "Final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "This is the user formulated question: {messages[0]['content']}.\n",
    "\n",
    "This is the response obtained from an weather API: {weather_conditions}.\n",
    "\n",
    "Provide a polite response to the user in English including the info found \n",
    "in the weather API response and include some clothing recommendations.\n",
    "\"\"\"\n",
    "\n",
    "# messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": prompt}\n",
    "]\n",
    "\n",
    "# call\n",
    "response = client.chat.completions.create(\n",
    "    messages = messages,\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipy_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
